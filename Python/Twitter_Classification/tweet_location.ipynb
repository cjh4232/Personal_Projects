{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Off-Platform Project: Classifying Tweets\n",
    "\n",
    "In this off-platform project, we will use a Naive Bayes Classifier to find patterns in real tweets. We have three files: `new_york.json`, `london.json`, and `paris.json`. These three files contain tweets that were gathered from those locations.\n",
    "\n",
    "The goal is to create a classification algorithm that can classify any tweet (or sentence) and predict whether that sentence came from New York, London, or Paris."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigate the Data\n",
    "\n",
    "To begin, let's take a look at the data. Let us import `new_york.json` and print the following information:\n",
    "* The number of tweets.\n",
    "* The columns, or features, of a tweet.\n",
    "* The text of the 12th tweet in the New York dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4723\nIndex(['created_at', 'id', 'id_str', 'text', 'display_text_range', 'source',\n       'truncated', 'in_reply_to_status_id', 'in_reply_to_status_id_str',\n       'in_reply_to_user_id', 'in_reply_to_user_id_str',\n       'in_reply_to_screen_name', 'user', 'geo', 'coordinates', 'place',\n       'contributors', 'is_quote_status', 'quote_count', 'reply_count',\n       'retweet_count', 'favorite_count', 'entities', 'favorited', 'retweeted',\n       'filter_level', 'lang', 'timestamp_ms', 'extended_tweet',\n       'possibly_sensitive', 'quoted_status_id', 'quoted_status_id_str',\n       'quoted_status', 'quoted_status_permalink', 'extended_entities',\n       'withheld_in_countries'],\n      dtype='object')\nBe best #ThursdayThoughts\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "new_york_tweets = pd.read_json(\"new_york.json\", lines=True)\n",
    "print(len(new_york_tweets))\n",
    "print(new_york_tweets.columns)\n",
    "print(new_york_tweets.loc[12][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll load the London and Paris tweets into DataFrames named `london_tweets` and `paris_tweets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5341\n2510\n"
     ]
    }
   ],
   "source": [
    "london_tweets = pd.read_json(\"london.json\", lines=True)\n",
    "paris_tweets = pd.read_json(\"paris.json\", lines=True)\n",
    "print(len(london_tweets))\n",
    "print(len(paris_tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying using language: Naive Bayes Classifier\n",
    "\n",
    "We're going to create a Naive Bayes Classifier! Let's begin by looking at the way language is used differently in these three locations. Let's grab the text of all of the tweets and make it one big list. In the code block below, we'll make a list of all the New York tweets, London tweets, & Paris, tweets. \n",
    "\n",
    "Then combine all three into a variable named `all_tweets`\n",
    "\n",
    "Let's also make the labels associated with those tweets. `0` represents a New York tweet, `1`  represents a London tweet, and `2` represents a Paris tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_york_text = new_york_tweets[\"text\"].tolist()\n",
    "london_text = london_tweets[\"text\"].tolist()\n",
    "paris_text = paris_tweets[\"text\"].tolist()\n",
    "\n",
    "all_tweets = new_york_text + london_text + paris_text\n",
    "labels = [0] * len(new_york_text) + [1] * len(london_text) + [2] * len(paris_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a Training and Test Set\n",
    "\n",
    "We can now break our data into a training set and a test set. We'll use scikit-learn's `train_test_split` function to do this split. This function takes two required parameters: It takes the data, followed by the labels. We'll set the optional parameter `test_size` to be `0.2`. Finally, set the optional parameter `random_state` to `1`. This will make it so our data is split in the same way. \n",
    "\n",
    "We'll store the results in variables named `train_data`, `test_data`, `train_labels`, and `test_labels`.\n",
    "\n",
    "Let's print the length of `train_data` and the length of `test_data` to check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train data length: 10059 \nTest data length: 2515\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(all_tweets, labels, test_size=0.2, random_state=1)\n",
    "\n",
    "print('Train data length: {} \\nTest data length: {}'.format(len(train_data), len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making the Count Vectors\n",
    "\n",
    "To use a Naive Bayes Classifier, we need to transform our lists of words into count vectors.\n",
    "\n",
    "To start, We'll create a `CountVectorizer` named `counter`.\n",
    "\n",
    "Next, we'll call the `.fit()` method using `train_data` as a parameter.\n",
    "\n",
    "Finally, let's transform `train_data` and `test_data` into Count Vectors using `train_data` as a parameter and store the result in `train_counts`. We'll do the same for `test_data` and store the result in `test_counts`.\n",
    "\n",
    "Let's print `train_data[3]` and `train_counts[3]` to see what a tweet looks like as a Count Vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "saying bye is hard. Especially when youre saying bye to comfort.\n  (0, 5022)\t2\n  (0, 6371)\t1\n  (0, 9552)\t1\n  (0, 12314)\t1\n  (0, 13903)\t1\n  (0, 23994)\t2\n  (0, 27146)\t1\n  (0, 29397)\t1\n  (0, 30274)\t1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "counter = CountVectorizer()\n",
    "counter.fit(train_data)\n",
    "train_counts = counter.transform(train_data)\n",
    "test_counts = counter.transform(test_data)\n",
    "\n",
    "print(train_data[3])\n",
    "print(train_counts[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test the Naive Bayes Classifier\n",
    "\n",
    "We now have the inputs to our classifier. Let's use the CountVectors to train and test the Naive Bayes Classifier!\n",
    "\n",
    "First, make a `MultinomialNB` named `classifier`.\n",
    "\n",
    "Next, let's call `classifier`'s `.fit()` method. This method takes two parameters &mdash; the training data and the training labels. `train_counts` contains the training data and `train_labels` containts the labels for that data.\n",
    "\n",
    "Calling `.fit()` calculates all of the probabilities used in Bayes Theorem. The model is now ready to quickly predict the location of a new tweet. \n",
    "\n",
    "Finally, let's test our model. `classifier`'s `.predict()` method using `test_counts` as a parameter and store the results in a variable named `predictions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(train_counts, train_labels)\n",
    "predictions = classifier.predict(test_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Our Model\n",
    "\n",
    "Now that the classifier has made its predictions, let's see how well it did. Let's look at two different ways to do this. First, we can call scikit-learn's `accuracy_score` function. This function should take two parameters &mdash;  the `test_labels` and our `predictions` and print the results. This prints the percentage of tweets in the test set that the classifier correctly classified.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.6779324055666004\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(accuracy_score(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other way we can evaluate our model is by looking at the **confusion matrix**. A confusion matrix is a table that describes how our classifier made its predictions. For example, if there were two labels, A and B, a confusion matrix might look like this:\n",
    "\n",
    "```\n",
    "9 1\n",
    "3 5\n",
    "```\n",
    "\n",
    "In this example, the first row shows how the classifier classified the true A's. It guessed that 9 of them were A's and 1 of them was a B. The second row shows how the classifier did on the true B's. It guessed that 3 of them were A's and 5 of them were B's.\n",
    "\n",
    "For our project using tweets, there were three classes &mdash; `0` for New York, `1` for London, and `2` for Paris. We can see the confustion matrix by printing the result of the `confusion_matrix` function using `test_labels` and `predictions` as parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[541 404  28]\n [203 824  34]\n [ 38 103 340]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Our Own Tweet\n",
    "\n",
    "The confusion matrix should line up with our intuition. The classifier predicts tweets that were actually from New York as either New York tweets or London tweets, but almost never Paris tweets. Similarly, the classifier rarely misclassifies the tweets that were actually from Paris. Tweets coming from two English speaking countries are harder to distinguish than tweets in different languages.\n",
    "\n",
    "Now it's our chance to write a tweet and see how the classifier works! Let's create a string and store it in a variable named `tweet`. \n",
    "\n",
    "We'll call `counter`'s `.transform()` method using `[tweet]` as a parameter and save the result as `tweet_counts`.\n",
    "\n",
    "Finally, we'll pass `tweet_counts` as parameter to `classifier`'s `.predict()` method and print the result. This should give us the prediction for the tweet. Remember a `0` represents New York, a `1` represents London, and a `2` represents Paris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "tweet = ['When people try to say neither side is focused on the people this is why I dont know wtf theyre talking about']\n",
    "\n",
    "tweet_counts = counter.transform(tweet)\n",
    "print(classifier.predict(tweet_counts))"
   ]
  },
  {
   "source": [
    "Woo hoo! It works since I'm from New York (state, not the city)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}